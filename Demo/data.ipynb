{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "Data description include source, size, type, attributes, modality, etc. \n",
    "Data retrieval from community data centers, personal cloud storage, or published datasets. \n",
    "Feature extraction and engineering.\n",
    "\n",
    "Model training data uses data from the following sources National Resource Conservation Service (NRCS) Snow Telemetry (SNOTEL), California Data Exchange Center (CDEC), Copernicus 90-m DEM, and the NASA Airborne Snow Observatory (ASO).\n",
    "In addition to these data sources, we create feature based upon the water year weak, latitude, longitude, and elevation.\n",
    "From these data sources we created the following features for input into the machine learning models.\n",
    "\n",
    "|Feature id| Description|\n",
    " |:-----------: | :--------: |\n",
    " |WY Week | Numerical ID of the week of the water year|\n",
    " |Latitude | Center latitude of the training grid cell |\n",
    " |Longitude | Center longitude of the training grid cell|\n",
    " |Elevation | DEM elevation of the training grid cell |\n",
    " |Northness | Calculated northness of the training cell|\n",
    " |SNOTEL SWE | Current week's observed SWE from SNOTEL|\n",
    " |Prev SNOTEL SWE | Previous week's observed SWE from SNOTEL|\n",
    " |Delta SNOTEL SWE | Difference between Previous week's and current week's observed SWE from SNOTEL|\n",
    " |Previous SWE | Observed SWE from previous week|\n",
    " \n",
    " *Note, the Previous SWE feature is an observation for model training and testing but will be predicted when forecating.*\n",
    " \n",
    " ## SNOTEL and CDEC Snow Monitoring Data\n",
    " \n",
    " <img align = 'right' src=\"Images/SNOTEL.jpg\" alt = 'drawing' width = '400'/>\n",
    "SNOTEL is an automated system of snowpack and related climate sensors operated by the Natural Resources Conservation Service (NRCS) that record key snow and hydrometeorological components and transmit observations via a telemetry netrwork. \n",
    "There are over 600 SNOTEL sites distributed accross the western US and Alaska, and have become standard climate information to understand snowpack dynamics and estimate water supply. \n",
    "Standard SNOTEL sites include a pressure sensing snow pillow to measure SWE based on hydrostatic pressure created by overlying snow, snow depth, storage precipitation gauge, and air temperature sensor.  \n",
    "System data-loggers provide functions for computing daily maximum, minimum, and average temperature information from data recorded every 15 minutes. \n",
    "\n",
    "SNOTEL central computer receives all monitoring station information and transmits to the Centralized Forecasting System (CFS). \n",
    "The CFS maintains the data is in a relational database, supporting analysis and graphics programs for data analytics.\n",
    "The system supports data retreival of current and historical data for analyses.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With Data Processing\n",
    "\n",
    "Data processing is likley the most tedious part of the model development pipeline. \n",
    "Model development leveraged multiple pre-processing and feature engineering steps to prepare the input data for use within the model.\n",
    "Data pre-processing included the use of elevation, slope, and aspect, derived from DEM data, through nearest-neighbor interpolation to produce values for all training, testing, and inference locations based on the four latitude and longitude corners of each respective grid cell.\n",
    "We calculated the northness metric based on the slope and aspect values of each grid cell.\n",
    "The embedded slope and aspect information within the northness metric aids in ML model training and reduces the overall dimensionality.\n",
    "\n",
    "Feature engineering consisted of the development of temporal features to support model training relative to seasonal snow accumulation and melt phases.\n",
    "Temporal features include a week-id for every observation and submission as an integer representing the week of the water year (WY) beginning October 1st, corresponding to WY week 1, and the week ending September 31st, corresponding to WY week 52.\n",
    "We utilized in-situ station SWE observations as features, using the values observed from the current (Snotel/CDEC SWE) and the previous week (Previous Snotel/CDEC SWE) as inputs.\n",
    "Using the observations from the current and previous week, we calculated the difference to capture the trend, either positive or negative, in SWE dynamics (i.e, melt or accumulation) with respect to each monitoring station ( $\\Delta$ SWE).\n",
    "Because of the serial correlation of snow accumulation and melt on the current timesteps SWE prediction, the model uses the SWE estimate of the previous week as a feature (Previous SWE).\n",
    "For model training and testing, the Previous SWE input is from NASA ASO datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pystac_client import Client\n",
    "import planetary_computer\n",
    "import rioxarray\n",
    "from pyproj import Transformer\n",
    "import h5py\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "#import geopandas\n",
    "import richdem as rd\n",
    "#import elevation\n",
    "#import hdfdict\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import io\n",
    "from datetime import date, timedelta\n",
    "import xarray as xr\n",
    "import requests\n",
    "#import netCDF4\n",
    "import tempfile\n",
    "import os\n",
    "from platform import python_version\n",
    "import tables\n",
    "\n",
    "from matplotlib.axes import Axes\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#print(pd.__version__) \n",
    "print(python_version()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "h5py                3.8.0\n",
       "matplotlib          3.2.2\n",
       "mpl_toolkits        NA\n",
       "numpy               1.21.6\n",
       "pandas              1.3.4\n",
       "planetary_computer  0.4.9\n",
       "pyproj              2.2.0\n",
       "pystac_client       0.5.1\n",
       "requests            2.28.2\n",
       "richdem             NA\n",
       "rioxarray           0.9.1\n",
       "session_info        1.0.0\n",
       "tables              3.7.0\n",
       "tqdm                4.64.1\n",
       "xarray              0.17.0\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "PIL                 9.4.0\n",
       "affine              2.4.0\n",
       "attr                22.2.0\n",
       "backcall            0.2.0\n",
       "certifi             2022.12.07\n",
       "charset_normalizer  3.0.1\n",
       "click               8.1.3\n",
       "colorama            0.4.6\n",
       "cycler              0.10.0\n",
       "cython_runtime      NA\n",
       "dateutil            2.8.2\n",
       "debugpy             1.6.6\n",
       "decorator           5.1.1\n",
       "entrypoints         0.4\n",
       "idna                3.4\n",
       "ipykernel           6.16.2\n",
       "jedi                0.18.2\n",
       "kiwisolver          1.4.4\n",
       "matplotlib_inline   0.1.6\n",
       "nt                  NA\n",
       "ntsecuritycon       NA\n",
       "numexpr             2.8.4\n",
       "osgeo               3.4.2\n",
       "packaging           23.0\n",
       "parso               0.8.3\n",
       "pickleshare         0.7.5\n",
       "pkg_resources       NA\n",
       "prompt_toolkit      3.0.36\n",
       "psutil              5.9.4\n",
       "pydantic            1.10.4\n",
       "pydev_ipython       NA\n",
       "pydevconsole        NA\n",
       "pydevd              2.9.5\n",
       "pydevd_file_utils   NA\n",
       "pydevd_plugins      NA\n",
       "pydevd_tracing      NA\n",
       "pygments            2.14.0\n",
       "pyparsing           3.0.9\n",
       "pystac              1.5.0\n",
       "pythoncom           NA\n",
       "pytz                2022.7.1\n",
       "pywin32_bootstrap   NA\n",
       "pywin32_system32    NA\n",
       "pywintypes          NA\n",
       "rasterio            1.2.10\n",
       "scipy               1.4.1\n",
       "six                 1.16.0\n",
       "storemagic          NA\n",
       "swig_runtime_data4  NA\n",
       "tornado             6.2\n",
       "traitlets           5.9.0\n",
       "typing_extensions   NA\n",
       "urllib3             1.26.14\n",
       "wcwidth             0.2.6\n",
       "win32api            NA\n",
       "win32com            NA\n",
       "win32security       NA\n",
       "zmq                 25.0.0\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             7.34.0\n",
       "jupyter_client      7.4.9\n",
       "jupyter_core        4.12.0\n",
       "-----\n",
       "Python 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:37:49) [MSC v.1916 64 bit (AMD64)]\n",
       "Windows-10-10.0.19041-SP0\n",
       "-----\n",
       "Session information updated at 2023-02-13 20:04\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Provided SWE Observations from Snowcast Showdown\n",
    "\n",
    "The project supported the participation of the Snowcast Showdown, and while we shared useful code for SNOTEL and CDEC data retrievel (needed for forecasting), the information was provided by the United State Bureau of Reclaimation. \n",
    "We provide the supported modeling materials and provide the respective data processing and feature engineering steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18130/18130 [00:43<00:00, 421.07it/s]\n"
     ]
    }
   ],
   "source": [
    "#Set up training DF with key metadata per site\n",
    "#All coordinates of 1 km polygon used to develop ave elevation, ave slope, ave aspect\n",
    "\n",
    "colnames = ['cell_id', 'Region', 'BR_Coord', 'UR_Coord', 'UL_Coord', 'BL_Coord']\n",
    "SWEdata = pd.DataFrame(columns = colnames, dtype=object)\n",
    "\n",
    "#Load training SWE data\n",
    "TrainSWE = pd.read_csv('Provided_Data/Prediction_Location_Observations.csv')\n",
    "#drop na and put into modeling df format\n",
    "TrainSWE = TrainSWE.melt(id_vars=[\"cell_id\"]).dropna()\n",
    "\n",
    "#May or may not need to melt data\n",
    "#Load Testing SWE locations\n",
    "TestSWE = pd.read_csv('Provided_Data/submission_format.csv')\n",
    "#drop na and put into modeling df format\n",
    "TestSWE = TestSWE.melt(id_vars=[\"cell_id\"]).dropna()\n",
    "\n",
    "#Load  SWE location data\n",
    "with open(\"Provided_Data/grid_cells.geojson\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "#load ground truth values(SNOTEL): training\n",
    "GM_Train = pd.read_csv('Provided_Data/ground_measures_train_features.csv')\n",
    "#drop na and put into modeling df format\n",
    "GM_Train = GM_Train.melt(id_vars=[\"station_id\"]).dropna()\n",
    "\n",
    "#load ground truth values (SNOTEL): Testing\n",
    "GM_Test = pd.read_csv('Provided_Data/ground_measures_test_features.csv')\n",
    "#drop na and put into modeling df format\n",
    "GM_Test = GM_Test.melt(id_vars=[\"station_id\"]).dropna()\n",
    "\n",
    "#load ground truth meta\n",
    "GM_Meta = pd.read_csv('Provided_Data/ground_measures_metadata.csv')\n",
    "\n",
    "#merge training ground truth location metadata with snotel data\n",
    "GM_Train = GM_Meta.merge(GM_Train, how='inner', on='station_id')\n",
    "GM_Train = GM_Train.set_index('station_id')\n",
    "GM_Train.rename(columns={'name': 'location', 'latitude': 'Lat', 'longitude': 'Long', 'value': 'SWE'}, inplace=True)\n",
    "\n",
    "\n",
    "#merge testing ground truth location metadata with snotel data\n",
    "GM_Test = GM_Meta.merge(GM_Test, how='inner', on='station_id')\n",
    "GM_Test = GM_Test.set_index('station_id')\n",
    "GM_Test.rename(columns={'name': 'location', 'latitude': 'Lat', 'longitude': 'Long', 'value': 'SWE'}, inplace=True)\n",
    "\n",
    "#Make a SWE Grid location DF\n",
    "for i in tqdm(range(len(data[\"features\"]))):\n",
    "    properties = data[\"features\"][i][\"properties\"]\n",
    "    location = data[\"features\"][i][\"geometry\"]\n",
    "    DFdata = [properties [\"cell_id\"],  properties [\"region\"],location [\"coordinates\"][0][0] ,\n",
    "             location [\"coordinates\"][0][1], location [\"coordinates\"][0][2], location [\"coordinates\"][0][3] ]\n",
    "    df_length = len(SWEdata)\n",
    "    SWEdata.loc[df_length] = DFdata\n",
    "    \n",
    "#Make SWE location and observation DF\n",
    "#Training\n",
    "#merge site location metadata with observations\n",
    "TrainSWE = TrainSWE.merge(SWEdata, how='inner', on='cell_id')\n",
    "TrainSWE = TrainSWE.set_index('cell_id')\n",
    "TrainSWE.rename(columns={'variable': 'Date', 'value': 'SWE'}, inplace=True)\n",
    "\n",
    "#Make sure Date is in datetime data type\n",
    "TrainSWE['Date'] = pd.to_datetime(TrainSWE['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Rockies as a demonstration region for the tutorial\n",
    "\n",
    "The Snowcast Showdown competition modeling domain is the entire western US.\n",
    "For the tutorial, we select a subset of the region to reduce the computational burden on the end user which speeds up model development.\n",
    "We transition the file architecture from csv's to HDF5 to reduce the amount of memory and speed up computations. \n",
    "You can read more about using HDF5 files [here](https://www.geeksforgeeks.org/hdf5-files-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Lat Long information\n",
    "#Bottom right coord\n",
    "TrainSWE[['BR_Coord_Long','BR_Coord_Lat']] = pd.DataFrame(TrainSWE.BR_Coord.tolist(), index= TrainSWE.index)\n",
    "\n",
    "#Upper right coord\n",
    "TrainSWE[['UR_Coord_Long','UR_Coord_Lat']] = pd.DataFrame(TrainSWE.UR_Coord.tolist(), index= TrainSWE.index)\n",
    "\n",
    "#Upper left coord\n",
    "TrainSWE[['UL_Coord_Long','UL_Coord_Lat']] = pd.DataFrame(TrainSWE.UL_Coord.tolist(), index= TrainSWE.index)\n",
    "\n",
    "#Bottom Left coord\n",
    "TrainSWE[['BL_Coord_Long','BL_Coord_Lat']] = pd.DataFrame(TrainSWE.BL_Coord.tolist(), index= TrainSWE.index)\n",
    "\n",
    "#For model tutorial, selecting the Northern Rockies (UCOL)\n",
    "TrainSWE = TrainSWE[TrainSWE['Region'] =='central rockies']\n",
    "SWEdata = SWEdata[SWEdata['Region'] =='central rockies']\n",
    "GM_Train = GM_Train[GM_Train['state'] =='Colorado']\n",
    "\n",
    "#Save Files \n",
    "TrainSWE.to_hdf('Provided_Data/SWE_Rockies.h5', key = 'TrainSWE', complevel = 9, complib = 'bzip2')\n",
    "SWEdata.to_hdf('Provided_Data/SWE_Rockies.h5', key = 'SWEdata', complevel = 9, complib = 'bzip2')\n",
    "GM_Train.to_hdf('Provided_Data/SWE_Rockies.h5', key = 'GM_Train', complevel = 9, complib = 'bzip2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Copernicus Geospatial Data - 90 m DEM\n",
    "\n",
    " <img align = 'right' src=\"./Images/DEM.jpg\" alt = 'drawing' width = '600'/>\n",
    " \n",
    "We use the copernicus 90 m DEM hosted by [Microsoft](https://planetarycomputer.microsoft.com) to provide the geospatial information for each training and testing location.\n",
    "For each corner of each grid, we get the slope, elevation, and aspect, and calculate the average value to form as average 1-km geospatial information.\n",
    "We use the following code to access the DEM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read SWE observational data (SNOTEL, CDEC, NASA ASO) and respective metadata into memory\n",
    "TrainSWE = pd.read_hdf('Provided_Data/SWE_Rockies.h5', 'TrainSWE')\n",
    "SWEdata = pd.read_hdf('Provided_Data/SWE_Rockies.h5', 'SWEdata')\n",
    "GM_Train = pd.read_hdf('Provided_Data/SWE_Rockies.h5',  'GM_Train')\n",
    "\n",
    "#Set up a framework to retrieve geospatial information for each site (elevation, weather, slope, aspect, etc)\n",
    "\n",
    "#Develop a DF to get each site's geospatial information \n",
    "geocols = [ 'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "       'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat']\n",
    "\n",
    "\n",
    "Geospatial_df = TrainSWE.copy()\n",
    "Geospatial_df['rowid'] = Geospatial_df.index\n",
    "Geospatial_df = Geospatial_df.drop_duplicates(subset = 'rowid')\n",
    "Geospatial_df = pd.DataFrame(Geospatial_df[geocols])\n",
    "\n",
    "#Define the AOI around the cell locations from clockwise\n",
    "\n",
    "area_of_interest = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            #lower left\n",
    "            [Geospatial_df['BL_Coord_Long'].min(), Geospatial_df['BL_Coord_Lat'].min()],\n",
    "            #upper left\n",
    "            [Geospatial_df['UL_Coord_Long'].min(), Geospatial_df['UL_Coord_Lat'].max()],\n",
    "            #upper right\n",
    "            [Geospatial_df['UR_Coord_Long'].max(), Geospatial_df['UR_Coord_Lat'].max()],\n",
    "            #lower right\n",
    "            [Geospatial_df['UR_Coord_Long'].max(), Geospatial_df['BR_Coord_Lat'].min()],\n",
    "            #lower left\n",
    "            [Geospatial_df['BL_Coord_Long'].min(), Geospatial_df['BL_Coord_Lat'].min()],\n",
    "        ]\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#Make a connection to get 90m Copernicus Digital Elevation Model (DEM) data with the Planetary Computer STAC API\n",
    "\n",
    "client = Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    ignore_conformance=True,\n",
    ")\n",
    "\n",
    "\n",
    "search = client.search(\n",
    "    collections=[\"cop-dem-glo-90\"],\n",
    "    intersects=area_of_interest\n",
    ")\n",
    "\n",
    "tiles = list(search.get_items())\n",
    "\n",
    "#Make a DF to connect locations with the larger data tile, and then extract elevations\n",
    "regions = []\n",
    "\n",
    "for i in tqdm(range(0, len(tiles))):\n",
    "    row = [i, tiles[i].id]\n",
    "    regions.append(row)\n",
    "regions = pd.DataFrame(columns = ['sliceID', 'tileID'], data = regions)\n",
    "regions = regions.set_index(regions['tileID'])\n",
    "del regions['tileID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes blocks can take some time as we are connecting the geospatial attributes of each corner of each grid, and then taking the average to get averaged grid geospatial attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added Long,Lat to get polygon points\n",
    "def GeoStat_func(i, Geospatial_df, regions, elev_L, slope_L, aspect_L, Long, Lat, tile):\n",
    "\n",
    "    # convert coordinate to raster value\n",
    "    lon = Geospatial_df.iloc[i][Long]\n",
    "    lat = Geospatial_df.iloc[i][Lat]\n",
    "\n",
    "    \n",
    "    \n",
    "    #connect point location to geotile\n",
    "    tileid = 'Copernicus_DSM_COG_30_N' + str(math.floor(lat)) + '_00_W'+str(math.ceil(abs(lon))) +'_00_DEM'\n",
    "    \n",
    "    indexid = regions.loc[tileid]['sliceID']\n",
    "    \n",
    "\n",
    "   #Assing region\n",
    "    signed_asset = planetary_computer.sign(tiles[indexid].assets[\"data\"])\n",
    "    #get elevation data in xarray object\n",
    "    elevation = rioxarray.open_rasterio(signed_asset.href)\n",
    "\n",
    "    #create copies to extract other geopysical information\n",
    "    #Create Duplicate DF's\n",
    "    slope = elevation.copy()\n",
    "    aspect = elevation.copy()\n",
    "        \n",
    "    \n",
    "    #transform projection\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", elevation.rio.crs, always_xy=True)\n",
    "    xx, yy = transformer.transform(lon, lat)\n",
    "    \n",
    "    #extract elevation values into numpy array\n",
    "    tilearray = np.around(elevation.values[0]).astype(int)\n",
    "\n",
    "    #set tile geo to get slope and set at rdarray\n",
    "    geo = (math.floor(float(lon)), 90, 0.0, math.ceil(float(lat)), 0.0, -90)\n",
    "    tilearray = rd.rdarray(tilearray, no_data = -9999)\n",
    "    tilearray.projection = 'EPSG:4326'\n",
    "    tilearray.geotransform = geo\n",
    "\n",
    "    #get slope, note that slope needs to be fixed, way too high\n",
    "    #get aspect value\n",
    "    slope_arr = rd.TerrainAttribute(tilearray, attrib='slope_degrees')\n",
    "    aspect_arr = rd.TerrainAttribute(tilearray, attrib='aspect')\n",
    "\n",
    "    #save slope and aspect information \n",
    "    slope.values[0] = slope_arr\n",
    "    aspect.values[0] = aspect_arr\n",
    "\n",
    "    # get point values from grid\n",
    "    #print(elevation.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    \n",
    "    \n",
    "    elev = round(elevation.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    slop = round(slope.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    asp = round(aspect.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    \n",
    "    # This line was causing problems, modified to above\n",
    "    #elev = round(elevation.sel(x=(xx,), y=yy, method=\"nearest\").values[0][0])\n",
    "    #slop = round(slope.sel(x=(xx,), y=yy, method=\"nearest\").values[0][0])\n",
    "    #asp = round(aspect.sel(x=(xx,), y=yy, method=\"nearest\").values[0][0])\n",
    "    \n",
    "    \n",
    "    #add point values to list\n",
    "    elev_L.append(elev)\n",
    "    slope_L.append(slop)\n",
    "    aspect_L.append(asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2946 [00:16<1:27:16,  1.78s/it]"
     ]
    }
   ],
   "source": [
    "BLelev_L = []\n",
    "BLslope_L = []\n",
    "BLaspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "[GeoStat_func(i, Geospatial_df, regions, BLelev_L, BLslope_L, BLaspect_L,\n",
    "                'BL_Coord_Long', 'BL_Coord_Lat', tiles) for i in tqdm(range(0, len(Geospatial_df)))]\n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Geospatial_df['BL_Elevation_m'] = BLelev_L\n",
    "Geospatial_df['BL_slope_Deg'] = BLslope_L\n",
    "Geospatial_df['BLaspect_L'] = BLaspect_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ULelev_L = []\n",
    "ULslope_L = []\n",
    "ULaspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "[GeoStat_func(i, Geospatial_df, regions, ULelev_L, ULslope_L, ULaspect_L,\n",
    "                'UL_Coord_Long', 'UL_Coord_Lat', tiles) for i in tqdm(range(0,len(Geospatial_df)))]\n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Geospatial_df['UL_Elevation_m'] = ULelev_L\n",
    "Geospatial_df['UL_slope_Deg'] = ULslope_L\n",
    "Geospatial_df['ULaspect_L'] = ULaspect_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URelev_L = []\n",
    "URslope_L = []\n",
    "URaspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "[GeoStat_func(i, Geospatial_df, regions, URelev_L, URslope_L, URaspect_L,\n",
    "                'UR_Coord_Long', 'UR_Coord_Lat', tiles) for i in tqdm(range(0,len(Geospatial_df)))]\n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Geospatial_df['UR_Elevation_m'] = URelev_L\n",
    "Geospatial_df['UR_slope_Deg'] = URslope_L\n",
    "Geospatial_df['URaspect_L'] = URaspect_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRelev_L = []\n",
    "BRslope_L = []\n",
    "BRaspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "[GeoStat_func(i, Geospatial_df, regions, BRelev_L, BRslope_L, BRaspect_L,\n",
    "                'BR_Coord_Long', 'BR_Coord_Lat', tiles) for i in tqdm(range(0,len(Geospatial_df)))]\n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Geospatial_df['BR_Elevation_m'] = BRelev_L\n",
    "Geospatial_df['BR_slope_Deg'] = BRslope_L\n",
    "Geospatial_df['BRaspect_L'] = BRaspect_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Geospatial data into SWE.h5 file\n",
    "Geospatial_df.to_hdf('Provided_Data/SWE_Rockies.h5', key = 'Geospatial_df', complevel = 9, complib = 'bzip2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Geospatial_df = pd.read_hdf('Provided_Data/SWE_Rockies.h5', key = 'Geospatial_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get monitoring station site geospatial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the AOI around the cell locations from clockwise\n",
    "\n",
    "area_of_interest = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            #lower left\n",
    "            [GM_Train['Long'].min(), GM_Train['Lat'].min()],\n",
    "            #upper left\n",
    "            [GM_Train['Long'].min(), GM_Train['Lat'].max()],\n",
    "            #upper right\n",
    "            [GM_Train['Long'].max(), GM_Train['Lat'].max()],\n",
    "            #lower right\n",
    "            [GM_Train['Long'].max(), GM_Train['Lat'].min()],\n",
    "            #lower left\n",
    "            [GM_Train['Long'].min(), GM_Train['Lat'].min()],\n",
    "        ]\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a connection to get 90m Copernicus Digital Elevation Model (DEM) data with the Planetary Computer STAC API\n",
    "\n",
    "client = Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    ignore_conformance=True,\n",
    ")\n",
    "\n",
    "\n",
    "search = client.search(\n",
    "    collections=[\"cop-dem-glo-90\"],\n",
    "    intersects=area_of_interest\n",
    ")\n",
    "\n",
    "tiles = list(search.get_items())\n",
    "\n",
    "#Make a DF to connect locations with the larger data tile, and then extract elevations\n",
    "regions = []\n",
    "\n",
    "for i in range(0, len(tiles)):\n",
    "    row = [i, tiles[i].id]\n",
    "    regions.append(row)\n",
    "regions = pd.DataFrame(columns = ['sliceID', 'tileID'], data = regions)\n",
    "regions = regions.set_index(regions['tileID'])\n",
    "del regions['tileID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all unique Snotel sites\n",
    "Snotel = GM_Train.copy()\n",
    "Snotel = Snotel.reset_index()\n",
    "Snotel = Snotel.drop_duplicates(subset = ['station_id'])\n",
    "Snotel = Snotel.reset_index(drop = True) \n",
    "Snotel['Region'] = 'other'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elev_L = []\n",
    "slope_L = []\n",
    "aspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "[GeoStat_func(i, Snotel, regions, elev_L, slope_L, aspect_L,\n",
    "                'Long', 'Lat', tiles) for i in tqdm(range(0,len(Snotel)))]\n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Snotel['elevation_m'] = elev_L\n",
    "Snotel['slope_deg'] = slope_L\n",
    "Snotel['aspect'] = aspect_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Geospatial data into SWE.h5 file\n",
    "Snotel.to_hdf('Provided_Data/SWE_Rockies.h5', key = 'Snotel', complevel = 9, complib = 'bzip2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataFrame and Engineer Features\n",
    "\n",
    "In this section, we begin building the DataFrames's, connect geospatial information, and process geospatial information via feature engineering.\n",
    "Essentially, the section connect geospatial information to training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mean Geospatial data\n",
    "def mean_Geo(df, geo):\n",
    "    BL = 'BL'+geo\n",
    "    UL = 'UL'+geo\n",
    "    UR = 'UR'+geo\n",
    "    BR = 'BR'+geo\n",
    "    \n",
    "    df[geo] = (df[BL] + df[UL]+ df[UR] + df[BR]) /4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get geaspatial means\n",
    "geospatialcols = ['_Coord_Long', '_Coord_Lat', '_Elevation_m', '_slope_Deg' , 'aspect_L']\n",
    "\n",
    "#Training data\n",
    "[mean_Geo(Geospatial_df, i) for i in geospatialcols]\n",
    "\n",
    "#list of key geospatial component means\n",
    "geocol = ['_Coord_Long','_Coord_Lat','_Elevation_m','_slope_Deg','aspect_L']\n",
    "TrainGeo_df = Geospatial_df[geocol].copy()\n",
    "\n",
    "#adjust column names to be consistent with snotel\n",
    "TrainGeo_df = TrainGeo_df.rename( columns = {'_Coord_Long':'Long', '_Coord_Lat':'Lat', '_Elevation_m': 'elevation_m',\n",
    "                               '_slope_Deg':'slope_deg' , 'aspect_L': 'aspect'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Modeling Domain into Sub-domains\n",
    "\n",
    " <img align = 'right' src=\"Images/CONUSsnow.jpg\" alt = 'drawing' width = '300'/>\n",
    " \n",
    "The Snowcast Showdown modeling domain covered the entire western US. \n",
    "Thus, because of differnces in snowpack characteristics (maritime, coastal transitional, intermountain, and continental) and regional climate patterns, we divided the original domain into 23 sub-domains. \n",
    "The figure is from Haegeli, P (2004), *Scale Analysis of avalanche activity on persistent snowpack weakness with respect to large-scale backcountry avalance forecasting.*\n",
    "For the the tutorial, we still need to perform the Region_id() function.\n",
    "However, we will focus on the Norther Colorado Rockies region, also refered to as the Upper Colorado River Basin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make Region identifier. The data already includes Region, but too many 'other' labels\n",
    "\n",
    "def Region_id(df):\n",
    "    \n",
    "    for i in tqdm(range(0, len(df))):\n",
    "\n",
    "         #Northern Colorado Rockies\n",
    "        if -109 <= df['Long'][i] <=-104.5 and 38.3 <= df['Lat'][i] <= 40.99:\n",
    "            loc = 'N_Co_Rockies'\n",
    "            df['Region'].iloc[i] = loc \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading all files into memory to begin DataFrame Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Files into memory\n",
    "TrainSWE = pd.read_hdf('Provided_Data/SWE_Rockies.h5', 'TrainSWE')\n",
    "SWEdata = pd.read_hdf('Provided_Data/SWE_Rockies.h5', 'SWEdata')\n",
    "GM_Train = pd.read_hdf('Provided_Data/SWE_Rockies.h5',  'GM_Train')\n",
    "Snotel = pd.read_hdf('Provided_Data/SWE_Rockies.h5',  'Snotel')\n",
    "#Read Files into memory\n",
    "#read Geospatial data into memory\n",
    "Geospatial_df = pd.read_hdf('Provided_Data/SWE_Rockies.h5', 'Geospatial_df')\n",
    "\n",
    "#remove region as replacing with higher resolution version\n",
    "del TrainSWE['Region']\n",
    "\n",
    "#Fix date variable\n",
    "GM_Train = GM_Train.rename(columns={'variable':'Date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get geaspatial means\n",
    "geospatialcols = ['_Coord_Long', '_Coord_Lat', '_Elevation_m', '_slope_Deg' , 'aspect_L']\n",
    "\n",
    "#Training data\n",
    "[mean_Geo(Geospatial_df, i) for i in geospatialcols]\n",
    "\n",
    "#list of key geospatial component means\n",
    "geocol = ['_Coord_Long','_Coord_Lat','_Elevation_m','_slope_Deg','aspect_L']\n",
    "TrainGeo_df = Geospatial_df[geocol].copy()\n",
    "\n",
    "#adjust column names to be consistent with snotel\n",
    "TrainGeo_df = TrainGeo_df.rename( columns = {'_Coord_Long':'Long', '_Coord_Lat':'Lat', '_Elevation_m': 'elevation_m',\n",
    "                               '_slope_Deg':'slope_deg' , 'aspect_L': 'aspect'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attach a region id for each location\n",
    "TrainGeo_df['Region'] = 'other'\n",
    "Snotel['Region'] = 'other'\n",
    "\n",
    "#Assign region to dataframes\n",
    "Region_id(TrainGeo_df)\n",
    "Region_id(Snotel)\n",
    "\n",
    "#Select the Upper Colorado River Basin - N_Co_Rockies\n",
    "TrainGeo_df = TrainGeo_df[TrainGeo_df['Region'] == 'N_Co_Rockies']\n",
    "Snotel = Snotel[Snotel['Region'] == 'N_Co_Rockies']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for slicing regions into regional DataFrames\n",
    "\n",
    "While this step is not needed for the tutorial, it supports the scaling to a larger doamain to ensure different regions are correctly classified. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset data by each region into dictionary\n",
    "RegionTrain = {name: TrainGeo_df.loc[TrainGeo_df['Region'] == name] for name in TrainGeo_df.Region.unique()}\n",
    "RegionSnotel  = {name: Snotel.loc[Snotel['Region'] == name] for name in Snotel.Region.unique()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure no test locations classified as other\n",
    "print('Training') \n",
    "#look at region training sites\n",
    "for i in RegionTrain.keys():\n",
    "    print('There are', len(RegionTrain[i]), ' training locations in ', i)\n",
    "    \n",
    "print('         ') \n",
    "print('SNOTEL') \n",
    "#look at region training sites\n",
    "for i in RegionSnotel.keys():\n",
    "    print('There are', len(RegionSnotel[i]), ' Snotel locations in ', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Geophysical Region information as a .pkl file\n",
    "Here, we save the geophysical region information as a .pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dictionaries as pkl\n",
    "# create a binary pickle file \n",
    "RTrain = open(\"Provided_Data/RegionTrain.pkl\",\"wb\")\n",
    "Rsnow = open(\"Provided_Data/RegionSnotel.pkl\",\"wb\")\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(RegionTrain,RTrain)\n",
    "pickle.dump(RegionSnotel,Rsnow)\n",
    "\n",
    "# close file\n",
    "RTrain.close()\n",
    "Rsnow.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load regionalized geospatial data\n",
    "RegionTrain = open(\"Provided_Data/RegionTrain.pkl\", \"rb\")\n",
    "RegionSnotel = open(\"Provided_Data/RegionSnotel.pkl\", \"rb\")\n",
    "\n",
    "RegionTrain = pickle.load(RegionTrain)\n",
    "RegionSnotel = pickle.load(RegionSnotel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the NASA ASO and SNOTEL sites\n",
    "It is important for any modeling exercise to ensure your data is in the location that you expect.\n",
    "The GeoPlot() function plots the respective observations over a map of the Rocky mountains to visualize the modeling domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This plots the location of all df data points\n",
    "\n",
    "def GeoPlot(df):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(12, 10)\n",
    "\n",
    "    #merc also works for projection # Cylindrical Equal Area. https://matplotlib.org/basemap/api/basemap_api.html#module-mpl_toolkits.basemap\n",
    "\n",
    "    m = Basemap(projection='cea', \\\n",
    "                llcrnrlat=35, urcrnrlat=42, \\\n",
    "                llcrnrlon=-112, urcrnrlon=-102, \\\n",
    "                lat_ts=20, \\\n",
    "                resolution='c')\n",
    "\n",
    "    m.bluemarble(scale=2)   # full scale will be overkill\n",
    "    m.drawcoastlines(color='white', linewidth=0.2)  # add coastlines\n",
    "\n",
    "\n",
    "    # draw coastlines, meridians and parallels.\n",
    "    #m.drawcoastlines()\n",
    "    m.drawcountries()\n",
    "    m.drawstates()\n",
    "    #m.drawmapboundary(fill_color='#99ffff')\n",
    "    #m.fillcontinents(color='#cc9966',lake_color='#99ffff')\n",
    "    m.drawparallels(np.arange(20,60,10),labels=[1,1,0,0])\n",
    "    m.drawmeridians(np.arange(-120,-90,10),labels=[0,0,0,1])\n",
    "\n",
    "\n",
    "    #Make unique color for each regions\n",
    "    number_of_colors = len(df.keys())\n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                 for i in range(number_of_colors)]\n",
    "\n",
    "    Location = list(df.keys())\n",
    "    colordict = {k: v for k, v in zip(Location, color)}\n",
    "\n",
    "\n",
    "    for i in df.keys():\n",
    "            x, y = m(np.array(df[i]['Long']), np.array(df[i]['Lat'])) \n",
    "            m.scatter(x, y, 10, marker='o', color=colordict[i], label = str(i)) \n",
    "\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "\n",
    "    plt.title('Training Locations')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeoPlot(RegionTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeoPlot(RegionSnotel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Geospatial data to SWE observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function connects stationary geospatial information to observations\n",
    "def Geo_to_Data(geodf, SWE, id):\n",
    "    dfcols = ['Long','Lat','elevation_m','slope_deg','aspect','Date','SWE','Region']\n",
    "    datadf = geodf.merge(SWE, how='inner', on=id)\n",
    "    #datadf = datadf.set_index(id)\n",
    "    datadf=datadf[dfcols]\n",
    "    return datadf\n",
    "\n",
    "#Create a temporal attribute, week_num(), that reflect the week id of the water year, beginning October 1st\n",
    "def week_num(df):\n",
    "        #week of water year\n",
    "    weeklist = []\n",
    "\n",
    "    for i in tqdm(range(0,len(df))):\n",
    "        if df['Date'][i].month<11:\n",
    "            y = df['Date'][i].year-1\n",
    "        else:\n",
    "            y = df['Date'][i].year\n",
    "            \n",
    "        WY_start = pd.to_datetime(str(y)+'-10-01')\n",
    "        deltaday = df['Date'][i]-WY_start\n",
    "        deltaweek = round(deltaday.days/7)\n",
    "        weeklist.append(deltaweek)\n",
    "\n",
    "\n",
    "    df['WYWeek'] = weeklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8816/8816 [00:01<00:00, 8007.20it/s]\n"
     ]
    }
   ],
   "source": [
    "#Merge Geospatial data to SWE observations\n",
    "\n",
    "Snotel = Snotel.rename(columns = {'slope_Deg': 'slope_deg'})\n",
    "Snotel.set_index('station_id', inplace = True)\n",
    "\n",
    "Training = Geo_to_Data(TrainGeo_df, TrainSWE, 'cell_id')\n",
    "\n",
    "# get snotel station id, region, slope, and aspect to merge with obervations\n",
    "Snocol = ['Region','slope_deg','aspect']\n",
    "Snotel = Snotel[Snocol]\n",
    "GM_Snotel_train = Geo_to_Data(Snotel, GM_Train, 'station_id')\n",
    "\n",
    "#Make Date in datetime dtype\n",
    "Training['Date'] = pd.to_datetime(Training['Date'])\n",
    "GM_Snotel_train['Date'] = pd.to_datetime(GM_Snotel_train['Date'])\n",
    "\n",
    "#add week number to observations\n",
    "week_num(Training)\n",
    "\n",
    "#Save Geospatial data into SWE.h5 file\n",
    "GM_Snotel_train.to_hdf('Provided_Data/SWE_Rockies.h5', key = 'GM_Snotel_train', complevel = 9, complib = 'bzip2')\n",
    "Training.to_hdf('Provided_Data/SWE_Rockies.h5', key = 'Training', complevel = 9, complib = 'bzip2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect observations to regional data\n",
    "#subset data by each region into dictionary\n",
    "RegionTrain = {name: Training.loc[Training['Region'] == name] for name in Training.Region.unique()}\n",
    "RegionSnotel_Train  = {name: GM_Snotel_train.loc[GM_Snotel_train['Region'] == name] for name in GM_Snotel_train.Region.unique()}\n",
    "\n",
    "\n",
    "#save dictionaries as pkl\n",
    "# create a binary pickle file \n",
    "RTrain = open(\"Provided_Data/RegionTrain.pkl\",\"wb\")\n",
    "Rsnow_Train = open(\"Provided_Data/RegionSnotel_Train.pkl\",\"wb\")\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(RegionTrain,RTrain)\n",
    "pickle.dump(RegionSnotel_Train,Rsnow_Train)\n",
    "\n",
    "# close file\n",
    "RTrain.close()\n",
    "Rsnow_Train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load regionalized geospatial data\n",
    "RTrain = open(\"Provided_Data/RegionTrain.pkl\",\"rb\")\n",
    "Rsnow_Train = open(\"Provided_Data/RegionSnotel_Train.pkl\",\"rb\")\n",
    "\n",
    "RegionTrain = pickle.load(RTrain)\n",
    "RegionSnotel_Train = pickle.load(Rsnow_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Northness Feature\n",
    "\n",
    " <img align = 'center' src=\"./Images/northness.JPG\" alt = 'drawing' width = '300'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function defines northness: :  sine(Slope) * cosine(Aspect). this gives you a northness range of -1 to 1.\n",
    "#Note you'll need to first convert to radians. \n",
    "#Some additional if else statements to get around sites with low obervations\n",
    "def northness(df):    \n",
    "    \n",
    "    if len(df) == 8: #This removes single value observations, need to go over and remove these locations from training too\n",
    "        #Determine northness for site\n",
    "        #convert to radians\n",
    "        df = pd.DataFrame(df).T\n",
    "        \n",
    "        df['aspect_rad'] = df['aspect']*0.0174533\n",
    "        df['slope_rad'] = df['slope_deg']*0.0174533\n",
    "        \n",
    "        df['northness'] = -9999\n",
    "        for i in range(0, len(df)):\n",
    "            df['northness'].iloc[i] = math.sin(df['slope_rad'].iloc[i])*math.cos(df['aspect_rad'].iloc[i])\n",
    "\n",
    "        #remove slope and aspects to clean df up\n",
    "        df = df.drop(columns = ['aspect', 'slope_deg', 'aspect_rad', 'slope_rad', 'Region'])\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    else:\n",
    "         #convert to radians\n",
    "        df['aspect_rad'] = df['aspect']*0.0174533\n",
    "        df['slope_rad'] = df['slope_deg']*0.0174533\n",
    "        \n",
    "        df['northness'] = -9999\n",
    "        for i in range(0, len(df)):\n",
    "            df['northness'].iloc[i] = math.sin(df['slope_rad'].iloc[i])*math.cos(df['aspect_rad'].iloc[i])\n",
    "\n",
    "        \n",
    "         #remove slope and aspects to clean df up\n",
    "        df = df.drop(columns = ['aspect', 'slope_deg', 'aspect_rad', 'slope_rad', 'Region'])\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 3/58 [00:00<00:02, 27.03it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6/58 [00:00<00:01, 26.46it/s]\u001b[A\n",
      " 16%|█▌        | 9/58 [00:00<00:01, 26.39it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 12/58 [00:00<00:01, 25.91it/s]\u001b[A\n",
      " 26%|██▌       | 15/58 [00:00<00:01, 25.97it/s]\u001b[A\n",
      " 34%|███▍      | 20/58 [00:00<00:01, 31.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 41%|████▏     | 24/58 [00:00<00:01, 29.75it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|████▋     | 27/58 [00:00<00:01, 28.89it/s]\u001b[A\n",
      " 52%|█████▏    | 30/58 [00:01<00:00, 28.19it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 33/58 [00:01<00:00, 28.08it/s]\u001b[A\n",
      " 62%|██████▏   | 36/58 [00:01<00:00, 27.62it/s]\u001b[A\n",
      " 67%|██████▋   | 39/58 [00:01<00:00, 27.81it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|███████▏  | 42/58 [00:01<00:00, 27.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████▊  | 45/58 [00:01<00:00, 26.96it/s]\u001b[A\n",
      " 83%|████████▎ | 48/58 [00:01<00:00, 26.84it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|████████▊ | 51/58 [00:01<00:00, 26.75it/s]\u001b[A\n",
      " 93%|█████████▎| 54/58 [00:01<00:00, 26.48it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 58/58 [00:02<00:00, 27.34it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#make northness feature and delete regions, slope, aspect features for each training and testing cell\n",
    "for i in tqdm(RegionTrain):\n",
    "    RegionTrain[i] = northness(RegionTrain[i])\n",
    "    \n",
    "#Make dictionary in Regions dict for each region's dictionary of Snotel sites\n",
    "Regions = list(RegionTrain.keys()).copy()\n",
    "\n",
    "#Make northness for all Snotel observations\n",
    "for i in tqdm(Regions):\n",
    "    \n",
    "    snotel = i+'_Snotel'\n",
    "    RegionTrain[snotel] = {site: RegionSnotel_Train[i].loc[site] for site in RegionSnotel_Train[i].index.unique()}\n",
    "    \n",
    "    #get training and testing sites that are the same\n",
    "    train = RegionTrain[snotel].keys()\n",
    "    \n",
    "    #make Northing metric\n",
    "    for j in tqdm(train):\n",
    "  #     \n",
    "        RegionTrain[snotel][j] = northness(RegionTrain[snotel][j])\n",
    "   \n",
    "    #remove items we do not need\n",
    "        RegionTrain[snotel][j] = RegionTrain[snotel][j].drop(columns = ['Long', 'Lat'])\n",
    "    #make date index\n",
    "        RegionTrain[snotel][j] = RegionTrain[snotel][j].set_index('Date')\n",
    "        \n",
    "    #rename columns to represent site info\n",
    "        colnames = RegionTrain[snotel][j].columns\n",
    "        print(colnames)\n",
    "        sitecolnames = [x +'_'+ j for x in colnames]\n",
    "        names = dict(zip(colnames, sitecolnames))\n",
    "        RegionTrain[snotel][j] = RegionTrain[snotel][j].rename(columns = names)\n",
    "    \n",
    "    #Remove unused columns\n",
    "    columns = list(RegionTrain[snotel].keys()).copy()\n",
    "    for col in columns:\n",
    "        if len(RegionTrain[snotel][col].columns) >4:\n",
    "            del RegionTrain[snotel][col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/58 [00:00<00:02, 25.86it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6/58 [00:00<00:02, 25.86it/s]\u001b[A\n",
      " 16%|█▌        | 9/58 [00:00<00:01, 26.38it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|██        | 12/58 [00:00<00:01, 26.45it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|██▌       | 15/58 [00:00<00:01, 26.40it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███▍      | 20/58 [00:00<00:01, 31.32it/s]\u001b[A\n",
      " 41%|████▏     | 24/58 [00:00<00:01, 29.46it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 27/58 [00:00<00:01, 28.61it/s]\u001b[A\n",
      " 52%|█████▏    | 30/58 [00:01<00:01, 27.93it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████▋    | 33/58 [00:01<00:00, 27.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|██████▏   | 36/58 [00:01<00:00, 26.90it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 39/58 [00:01<00:00, 26.59it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 42/58 [00:01<00:00, 26.64it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████▊  | 45/58 [00:01<00:00, 26.34it/s]\u001b[A\n",
      " 83%|████████▎ | 48/58 [00:01<00:00, 26.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|████████▊ | 51/58 [00:01<00:00, 26.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 93%|█████████▎| 54/58 [00:01<00:00, 26.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 58/58 [00:02<00:00, 26.91it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n",
      "Index(['elevation_m', 'SWE', 'northness'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.41it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm(Regions):\n",
    "    \n",
    "    snotel = i+'_Snotel'\n",
    "    RegionTrain[snotel] = {site: RegionSnotel_Train[i].loc[site] for site in RegionSnotel_Train[i].index.unique()}\n",
    "    \n",
    "    #get training and testing sites that are the same\n",
    "    train = RegionTrain[snotel].keys()\n",
    "    \n",
    "    #make Northing metric\n",
    "    for j in tqdm(train):\n",
    "  #     \n",
    "        RegionTrain[snotel][j] = northness(RegionTrain[snotel][j])\n",
    "   \n",
    "    #remove items we do not need\n",
    "        RegionTrain[snotel][j] = RegionTrain[snotel][j].drop(columns = ['Long', 'Lat'])\n",
    "    #make date index\n",
    "        RegionTrain[snotel][j] = RegionTrain[snotel][j].set_index('Date')\n",
    "        \n",
    "    #rename columns to represent site info\n",
    "        colnames = RegionTrain[snotel][j].columns\n",
    "        print(colnames)\n",
    "        sitecolnames = [x +'_'+ j for x in colnames]\n",
    "        names = dict(zip(colnames, sitecolnames))\n",
    "        RegionTrain[snotel][j] = RegionTrain[snotel][j].rename(columns = names)\n",
    "    \n",
    "    #Remove unused columns\n",
    "    columns = list(RegionTrain[snotel].keys()).copy()\n",
    "    for col in columns:\n",
    "        if len(RegionTrain[snotel][col].columns) >4:\n",
    "            del RegionTrain[snotel][col]\n",
    "\n",
    "            \n",
    "#make a df for training each region, \n",
    "for R in tqdm(Regions):\n",
    "    snotels = R+'_Snotel'\n",
    "    RegionTrain[R] = RegionTrain[R].reset_index()\n",
    "    RegionTrain[R] = RegionTrain[R].set_index('Date')\n",
    "      \n",
    "    for S in RegionTrain[snotels]:\n",
    "        RegionTrain[R]= pd.concat([RegionTrain[R], RegionTrain[snotels][S].reindex(RegionTrain[R].index)], axis=1)\n",
    "    \n",
    "    RegionTrain[R] = RegionTrain[R].fillna(-9999)\n",
    "    \n",
    "#save dictionaries as pkl\n",
    "# create a binary pickle file \n",
    "#load regionalized geospatial data\n",
    "RTrain = open(\"Provided_Data/Training.pkl\",\"wb\")\n",
    "\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(RegionTrain,RTrain)\n",
    "\n",
    "\n",
    "# close file\n",
    "RTrain.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Previous Week's SNOTEL SWE\n",
    "\n",
    "We use in-situ station SWE observations as features, using the values observed from the current (Snotel/CDEC SWE) and the previous week (Previous Snotel/CDEC SWE) as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prev_SWE_Snotel_Dict(DF, region):\n",
    "   # print(region)\n",
    "    \n",
    "    regionsnotel = region+'_Snotel'\n",
    "    \n",
    "    sites = DF[regionsnotel].keys()\n",
    "    \n",
    "    #week delta  \n",
    "    weekdelta = pd.Timedelta(7, \"d\")\n",
    "    \n",
    "    for i in sites:\n",
    "      #  print(i)\n",
    "        prevSWE = 'Prev_SWE_' + i\n",
    "        SWE = 'SWE_'+i\n",
    "        \n",
    "        DF[regionsnotel][i][prevSWE] = -9999.99\n",
    "        \n",
    "        #need to find the number of columns for ifelse\n",
    "        dfcols = len(DF[regionsnotel][i].columns)\n",
    "    \n",
    "\n",
    "        #if only one observation need to fix\n",
    "        if len(DF[regionsnotel][i]) == 1:\n",
    "            DF[regionsnotel][i] = DF[regionsnotel][i].T\n",
    "\n",
    "        for cell in range(1,len(DF[regionsnotel][i])):\n",
    "\n",
    "            if DF[regionsnotel][i].index[cell] - DF[regionsnotel][i].index[cell-1] == weekdelta:     \n",
    "                DF[regionsnotel][i][prevSWE][cell] = DF[regionsnotel][i][SWE][cell-1]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(Regions):\n",
    "    Prev_SWE_Snotel_Dict(RegionTrain, i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Delta SNOTEL SWE\n",
    "\n",
    "\n",
    "Using the observations from the current and previous week, we calculate the difference to capture the trend, either positive or negative, in SWE dynamics (i.e, melt or accumulation) with respect to each monitoring station ( $\\Delta$ SWE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Delta_SWE_Snotel_Dict(DF, region):\n",
    "    # print(region)\n",
    "    \n",
    "    regionsnotel = region+'_Snotel'\n",
    "    \n",
    "    sites = DF[regionsnotel].keys()\n",
    "    \n",
    "    for i in sites:\n",
    "      #  print(i)\n",
    "        prevSWE = 'Prev_SWE_' + i\n",
    "        SWE = 'SWE_'+i\n",
    "        Delta_SWE = 'Delta_'+SWE\n",
    "        \n",
    "        DF[regionsnotel][i][Delta_SWE] = DF[regionsnotel][i][SWE] - DF[regionsnotel][i][prevSWE]\n",
    "        DF[regionsnotel][i].loc[DF[regionsnotel][i][Delta_SWE]>150, Delta_SWE] =-9999.99\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.00it/s]\n"
     ]
    }
   ],
   "source": [
    "#Add Delta SWE feature\n",
    "for i in tqdm(Regions):\n",
    "    Delta_SWE_Snotel_Dict(RegionTrain, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Snotel Observations to NASA ASO\n",
    "Connect dataframe of NASA ASO with snotel observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "#make a df for training each region, \n",
    "#This DF comes last\n",
    "for R in tqdm(Regions):\n",
    "    snotels = R+'_Snotel'\n",
    "    RegionTrain[R] = RegionTrain[R].reset_index()\n",
    "    RegionTrain[R] = RegionTrain[R].set_index('Date')\n",
    "    \n",
    "    for S in RegionTrain[snotels]:\n",
    "        RegionTrain[R]= pd.concat([RegionTrain[R], RegionTrain[snotels][S].reindex(RegionTrain[R].index)], axis=1)\n",
    "    \n",
    "    RegionTrain[R] = RegionTrain[R].fillna(-9999)\n",
    "\n",
    "#Remove unnecessary features\n",
    "for region in Regions:\n",
    "    RegionTrain[region] = RegionTrain[region].drop( RegionTrain[region].filter(regex='elevation_m_').columns, axis=1)\n",
    "    RegionTrain[region] = RegionTrain[region].drop( RegionTrain[region].filter(regex='northness_').columns, axis=1)\n",
    "    RegionTrain[region] = RegionTrain[region].T.drop_duplicates().T\n",
    "#Save near complete DataFrame    \n",
    "RegionTrain['N_Co_Rockies'].to_hdf('Provided_Data/SWE_Rockies.h5', key = 'Delta_Prev_SWE_Training', complevel = 9, complib = 'bzip2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Site's Previous SWE \n",
    "\n",
    "Because of the serial correlation of snow accumulation and melt on the current timesteps SWE prediction, the model uses the SWE estimate of the previous week as a feature (Previous SWE).\n",
    "For model training and testing, the Previous SWE input is from NASA ASO datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Prev_SWE(df, region, outfile):    \n",
    "    print(region)\n",
    "    \n",
    "    df = df[region].reset_index()\n",
    "    df = df.set_index('cell_id')\n",
    "    \n",
    "    #week delta  \n",
    "    weekdelta = pd.Timedelta(7, \"d\")\n",
    "\n",
    "    #set up column for previous weeks SWE\n",
    "    df['prev_SWE'] = -9999.99\n",
    "    \n",
    "    #need to find the number of columns for ifelse\n",
    "    dfcols = len(df.columns)\n",
    "    \n",
    "    #Run through each uniqe site/cell id to calculate previous weeks SWE and add to a new dataframe\n",
    "    new_df = pd.DataFrame(columns = df.columns)\n",
    "    \n",
    "    #find unique sites\n",
    "    sites = df.index.unique()\n",
    "    \n",
    "    #regiondata = dict(zip(sites, new_df))\n",
    "   # print(regiondata.keys())\n",
    "    \n",
    "    for i in tqdm(sites):\n",
    "        site = df.loc[i].copy()\n",
    "\n",
    "        #if only one observation need to fix\n",
    "        if site.shape == (dfcols,):# and len(site) < 162:\n",
    "            #print(site, site.shape)\n",
    "            site = site.to_frame().T\n",
    "\n",
    "        for cell in range(1,len(site)):\n",
    "            if site['Date'][cell] - site['Date'][cell-1] == weekdelta:     \n",
    "                site['prev_SWE'][cell] = site['SWE'][cell-1]\n",
    "        dflist = [new_df, site]\n",
    "        new_df = pd.concat(dflist)\n",
    "        #regiondata[i] = site\n",
    "    new_df = new_df.fillna(-9999)\n",
    "    \n",
    "    #Put Prev_SWE next to SWE to confirm operations\n",
    "    prev_SWE = new_df['prev_SWE'].copy()\n",
    "    del new_df['prev_SWE']\n",
    "    new_df.insert(loc = 5,\n",
    "          column = 'prev_SWE',\n",
    "          value = prev_SWE)\n",
    "    \n",
    "    \n",
    "    new_df.to_hdf(outfile, key = region, complevel = 9, complib = 'bzip2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_Co_Rockies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [00:21<00:00, 83.50it/s]\n"
     ]
    }
   ],
   "source": [
    "#Choose a file path to save the final dataframe\n",
    "file = 'Provided_Data/Final_Training_DF.h5'\n",
    "\n",
    "#Run the previous SWE function and save the dataframe, you will now be ready to train the model.\n",
    "for region in Regions:\n",
    "    Prev_SWE(RegionTrain, region, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add model scaling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSM_37Rio",
   "language": "python",
   "name": "nsm_37rio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c446eef832ec964573dc49f36fd16bdbed40cbfbefbf557bc2dc78d9e7968689"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
